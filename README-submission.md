# Team #9
***

## Team Members

Last Name       | First Name      | GitHub User Name
--------------- | --------------- | --------------------
Caplinger       | Derek           | drc3141
Youngberg       | Matt            | mattyoungberg

## Test Results
**Q: How many of the dumpfiles matched (using the check-dumpfiles.sh script)?**  
A: All the dumpfiles matched üëç

**Q: How many of the query files results matched (using the check-queries.sh script)?**  
A: All the query files matched üëç

## Cache Performance Results
- No SQLite database created
- Ran on a Macbook with an M1 standard chip and 8GB of memory
- [hyperfine](https://github.com/sharkdp/hyperfine) used to measure average runtimes

| gbk file | degree | sequence length | cache | cache size | cache hit rate | run time |
| -------- | ------ | --------------- | ----- | ---------- | -------------- | -------- |
| test5.gbk|  102   |     20          |  no   |    0       |      0%        |  22.82s  |
| test5.gbk|  102   |     20          |  yes  |    100     |      66.49%    |  16.65s  |
| test5.gbk|  102   |     20          |  yes  |    500     |      77.90%    |  11.21s  |
| test5.gbk|  102   |     20          |  yes  |    1000    |      81.51%    |  10.54s  |
| test5.gbk|  102   |     20          |  yes  |    5000    |      95.17%    |   4.50s  |
| test5.gbk|  102   |     20          |  yes  |    10000   |      99.67%    |   2.18s  |



## AWS Notes

Deploying a default EC2 micro instance via the AWS console was straightforward. Initially, we faced difficulties
locating the instance online when it wasn't publicly exposed, which might have required using Amazon's custom DNS.
However, we instead opted to expose the instance to the public internet. We found the box immediately. Our familiarity
with SSH and key management quickly got us into the box and the repo pulled from GitHub.

After that, testing proceeded smoothly. We noticed that the `test0.gbk` and `test5.gbk` files ran slower compared to our
personal machines with more computing power, which was to be expected. Intrigued, though, we decided to run a test on
the `hs_ref_chrY.gbk` file. For this, we used `tmux` to detach the process, allowing us to disconnect and revisit it a
few hours later. Upon our return, it was evident that the process had stalled. A check on the system's resources
revealed that CPU, memory, and disk usage were within normal limits. Our hypothesis is that the JVM's allocated virtual
memory was exhausted, leading to the process hang-up, as indicated by a static, round memory usage figure. Concluding
that allocating additional virtual resources to the JVM was beyond our project's scope, and that 1GB of system memory
was not much to begin with, we decided not to pursue testing of that file further.

## Reflections

### Reflection: Derek Caplinger

This project was very challenging and there was, somehow, even more to it than I thought there would be during my initial read through of the project specifications. It was fun seeing how we can use the knowledge that we have in order to think a bit farther than just being a code monkey and only considering the immediate tasks requested and considering what other tools we have that make it easier to deliver what is being asked. I also enjoyed seeing a project that went beyond coding just to code and can see how what we are doing applies and can be used in any number of different fields of study.  

While I certainly learned quite a bit regarding the BTree data structure I think a lot more of what I have taken away from this project is the significant difference between working on a project on my own and working on something with other people. 

There was a lot for me to catch up on with using Github as part of a group in a repository with multiple people accessing it to make and upload changes. I also learned much more about the importance of communication and preparing for a project before getting started. For example a lot of time and probably headaches for the group, could have been saved if we discussed a more about the type of code styling, which admittedly should be pretty close across the board. In having these types of conversations ahead of time we likely would not have ahd to spend as much time double checking documentation and housekeeping steps as we did. 

I have also had first hand experience with a number of things I need to change about the way I complete a project and the benefits of communicating with others through the process. I have developed a few bad habits that I had figured I could change when needed, but this project forced me to confront that it may not be as easy as I thought to do so especially with a deadline creeping closer. I have developed a bad habit of pushing out documentation and formatting to after the code is working. This has caused a few moments were something could have been more clear what was happening and created less frustration with my peers. 

I have benefited greatly from being able to work with another person and get their insight into how a project like this can be done and in doing so feel that this project has helped me understand more clearly what I need to work on and how i want to improve moving forward. 
### Reflection: Matt Youngberg

This project has been the most intensive experience I've had in a class at Boise State, presenting several enjoyable
challenges.

One such challenge was implementing in-order traversal of the BTree, which I addressed through an Iterator
implementation. This approach provided an interface familiar to my teammates, and that in itself facilitated easy
parallel integration. The essence of an Iterator is to traverse a collection, and its use in this context was aptly
fitting. A significant advantage of using well-known interfaces is the reduced integration time due to established
familiarity. I‚Äôm glad that I could effectively utilize this aspect in our project. Adhering to the interface may have
made the code more verbose, as an Iterator almost always has to maintain state between invocations of next() if it wants
to be asymptotically efficient, but the benefits of using an established interface outweighed the verbosity. I used the
same approach with the GeneBank files as well, treating the files as a type of subsequence collection once the desired
subsequence length is known.

Another fun aspect of this project was all the facets that were available for optimization. How you define your nodes
on disk, how you manage your buffers, how you manage your cache, how you manage reading subsequences from disk, etc. It
seemed that everywhere you looked, there was an opportunity to optimize the code's runtime and memory footprint. And you
can bet that I took every opportunity I found to do so. I like to think it paid off, as our code performed very
similarly to the professors' solution, with and without a cache. It also looks as if we found similarly efficient
disk-encoding schemes, as their performance table suggested they could produce a BTree of with a minimum degree of 102
when given block sizes of 4096B, and ours could do the same.

Admittedly, there were moments of frustration while working on this project, particularly when it felt like we were
piecing together an already-complete project that had been abruptly "dropped out" from beneath us. This "fill in the
blanks" approach was frustrating in a few different scenarios:

- Some existing tests required us to introduce methods within our BTree solely for white box testing, and these methods
were not part of the initially provided interface. I felt that the black-box tests in conjunction with the script
integration tests were adequate to ensure system correctness, and so the given white-box tests felt unnecessary and 
forced me to add further methods and logic to the BTree that I would not have otherwise added.
- In our project, as guided by the provided tests, the constructors were tasked not only with in-memory entity creation
but also with comprehending their format on disk and dealing with different file states. From my experience, it's more
effective to separate in-memory entity creation (aka, constructors) from its reconstitution from a persistent store. The
in-memory construction process should be independently viable and accommodate varying states. Ideally, the understanding
of the disk format would be encapsulated within a static factory method. This method would handle the loading of
necessary data and subsequently use a constructor to build the entity in memory. Utilizing Java's capability for
constructor overloading is beneficial in this context: It allows the definition of a flexible, non-public constructor
that can accommodate various states of an entity, while public overloaded constructors can present simpler interfaces
focused on efficiently creating new entities. But the constructors as defined by the tests forced our hand.
- Exposing TreeObjects through the BTree interface seemed to compromise user-facing abstraction and simplicity,
principles I personally find crucial in design. In one sense, it made sense because frequencies were needed and
combining the subsequence and the frequency together necessitates some object if it's going to come from the BTree, but
in another, it also meant the client now had to assume more cognitive load to comprehend the custom object (whereas the
concept of a BTree in isolation is generally known and its methodset predictable). What seems a normal design to me is
for the onus to be on the client (for example, the command line programs) to check for duplicates before inserting into
a collection, and maintain a separate frequency count if needed for the sake of their application.

These experiences underscored a unique challenge in academic software projects: balancing the fulfillment of application 
requirements with internal coherence, particularly when developing a codebase from scratch that inherits complex design
choices from an existing solution.

Throughout our project, I often contemplated a question posed by Professor Amit regarding the feasibility of creating a
generic BTree. Reflecting on BTrees' common usage in relational databases, I recognized that they are optimized for
predefined data types with known disk sizes, a critical factor for efficient storage and retrieval. This optimization is
a key reason for the widespread use of BTree indexes in relational databases.

In contrast, object databases rarely employ BTrees due to the variable sizes of objects, which are not as easily
predictable. In the Java context, developing a generic BTree class poses the complex challenge of defining an abstract
data type that, when serialized to disk, maintains a consistent and pre-determined size. My experience suggests that
implementing such a system in Java would require extensions to accommodate all anticipated data types. Furthermore, any
addition of new types would necessitate further development work. Therefore, much like a relational database, a Java
application would need to limit its support to a predefined set of types, as accommodating additional types would
require significant development effort.

Ultimately, I enjoyed this project immensely. It had its fair share of frustrations, an exciting amount of big
challenges, and a great opportunity for growth as a computer scientist. I'm glad to have had the opportunity to work
with my teammate, and I'm proud of the work we accomplished.
